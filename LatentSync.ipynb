{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xanny1111/Spring_2025-Plotnov-24-VMz-/blob/master/LatentSync.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREPARE ENVIRONMENT**\n",
        "- Due to changes in some dependencies, you will be prompted to restart the session after all the required libraries are installed. Simply restart and run this section again."
      ],
      "metadata": {
        "id": "QK_OVw3gVDCZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EpLZhSJmFMBJ"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "!pip install diffusers mediapipe transformers huggingface-hub omegaconf einops opencv-python face-alignment decord ffmpeg-python safetensors soundfile\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "if not os.path.exists(\"LatentSync\"):\n",
        "    !git clone https://github.com/Isi-dev/LatentSync\n",
        "%cd LatentSync\n",
        "\n",
        "from google.colab import files\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "from diffusers import AutoencoderKL, DDIMScheduler\n",
        "from latentsync.models.unet import UNet3DConditionModel\n",
        "from latentsync.pipelines.lipsync_pipeline import LipsyncPipeline\n",
        "from latentsync.whisper.audio2feature import Audio2Feature\n",
        "from diffusers.utils.import_utils import is_xformers_available\n",
        "from accelerate.utils import set_seed\n",
        "import ipywidgets as widgets\n",
        "\n",
        "os.makedirs(\"/root/.cache/torch/hub/checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "model_urls = {\n",
        "    \"/root/.cache/torch/hub/checkpoints/s3fd-619a316812.pth\":\n",
        "        \"https://huggingface.co/Isi99999/LatentSync/resolve/main/auxiliary/s3fd-619a316812.pth\",\n",
        "    \"/root/.cache/torch/hub/checkpoints/2DFAN4-cd938726ad.zip\":\n",
        "        \"https://huggingface.co/Isi99999/LatentSync/resolve/main/auxiliary/2DFAN4-cd938726ad.zip\",\n",
        "    \"checkpoints/latentsync_unet.pt\":\n",
        "        \"https://huggingface.co/Isi99999/LatentSync/resolve/main/latentsync_unet.pt\",\n",
        "    \"checkpoints/tiny.pt\":\n",
        "        \"https://huggingface.co/Isi99999/LatentSync/resolve/main/whisper/tiny.pt\",\n",
        "    \"checkpoints/diffusion_pytorch_model.safetensors\":\n",
        "        \"https://huggingface.co/stabilityai/sd-vae-ft-mse/resolve/main/diffusion_pytorch_model.safetensors\",\n",
        "    \"checkpoints/config.json\":\n",
        "        \"https://huggingface.co/stabilityai/sd-vae-ft-mse/raw/main/config.json\",\n",
        "}\n",
        "\n",
        "for file_path, url in model_urls.items():\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Downloading {file_path} ...\")\n",
        "        subprocess.run([\"wget\", url, \"-O\", file_path], check=True)\n",
        "    else:\n",
        "        print(f\"File {file_path} already exists. Skipping download.\")\n",
        "\n",
        "print(\"Setup complete.\")\n",
        "\n",
        "def perform_inference(video_path, audio_path, seed=1247, num_inference_steps=20, guidance_scale=1.0, output_path=\"output_video.mp4\"):\n",
        "    config_path = \"configs/unet/first_stage.yaml\"\n",
        "    inference_ckpt_path = \"checkpoints/latentsync_unet.pt\"\n",
        "\n",
        "    config = OmegaConf.load(config_path)\n",
        "\n",
        "    is_fp16_supported = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] > 7\n",
        "    dtype = torch.float16 if is_fp16_supported else torch.float32\n",
        "\n",
        "    scheduler = DDIMScheduler.from_pretrained(\"configs\")\n",
        "\n",
        "    whisper_model_path = \"checkpoints/tiny.pt\"\n",
        "    audio_encoder = Audio2Feature(model_path=whisper_model_path, device=\"cuda\", num_frames=config.data.num_frames)\n",
        "\n",
        "    vae = AutoencoderKL.from_pretrained(\"checkpoints\", torch_dtype=dtype, local_files_only=True)\n",
        "    vae.config.scaling_factor = 0.18215\n",
        "    vae.config.shift_factor = 0\n",
        "\n",
        "    unet, _ = UNet3DConditionModel.from_pretrained(\n",
        "        OmegaConf.to_container(config.model),\n",
        "        inference_ckpt_path,\n",
        "        device=\"cpu\",\n",
        "    )\n",
        "\n",
        "    unet = unet.to(dtype=dtype)\n",
        "\n",
        "    if is_xformers_available():\n",
        "        unet.enable_xformers_memory_efficient_attention()\n",
        "        print('x_formers available!')\n",
        "\n",
        "    pipeline = LipsyncPipeline(\n",
        "        vae=vae,\n",
        "        audio_encoder=audio_encoder,\n",
        "        unet=unet,\n",
        "        scheduler=scheduler,\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    set_seed(seed)\n",
        "\n",
        "    pipeline(\n",
        "        video_path=video_path,\n",
        "        audio_path=audio_path,\n",
        "        video_out_path=output_path,\n",
        "        video_mask_path=output_path.replace(\".mp4\", \"_mask.mp4\"),\n",
        "        num_frames=config.data.num_frames,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        weight_dtype=dtype,\n",
        "        width=config.data.resolution,\n",
        "        height=config.data.resolution,\n",
        "    )\n",
        "    return output_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RUN IMAGE TO VIDEO**"
      ],
      "metadata": {
        "id": "FQe7fJHzbOmX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd2fad44"
      },
      "source": [
        "# Укажите имена ваших файлов, которые вы загрузили в папку LatentSync/checkpoints\n",
        "image_file_name = \"head1111.png\"  # <-- Измените на имя вашего файла изображения\n",
        "audio_file_name = \"1 part.WAV\"  # <-- Измените на имя вашего аудиофайла\n",
        "\n",
        "# --- Не изменяйте код ниже, если вы не знаете, что делаете ---\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torchaudio\n",
        "import subprocess\n",
        "from IPython.display import Video, display\n",
        "import gc\n",
        "\n",
        "# Definitions of helper functions (moved from cell t0fOMU7dZ4Qx)\n",
        "def convert_video_fps(input_path, target_fps):\n",
        "    if not os.path.exists(input_path) or os.path.getsize(input_path) == 0:\n",
        "        print(f\"Error: The video file {input_path} is missing or empty.\")\n",
        "        return None\n",
        "\n",
        "    output_path = f\"converted_{target_fps}fps.mp4\"\n",
        "\n",
        "    audio_check_cmd = [\n",
        "        \"ffprobe\", \"-i\", input_path, \"-show_streams\", \"-select_streams\", \"a\",\n",
        "        \"-loglevel\", \"error\"\n",
        "    ]\n",
        "    audio_present = subprocess.run(audio_check_cmd, capture_output=True, text=True).stdout.strip() != \"\"\n",
        "\n",
        "    cmd = [\n",
        "        \"ffmpeg\", \"-y\", \"-i\", input_path,\n",
        "        \"-filter:v\", f\"fps={target_fps}\",\n",
        "        \"-c:v\", \"libx264\", \"-preset\", \"fast\", \"-crf\", \"18\",\n",
        "    ]\n",
        "\n",
        "    if audio_present:\n",
        "        cmd.extend([\"-c:a\", \"aac\", \"-b:a\", \"192k\"])\n",
        "    else:\n",
        "        cmd.append(\"-an\")\n",
        "\n",
        "    cmd.append(output_path)\n",
        "\n",
        "    subprocess.run(cmd, check=True)\n",
        "    print(f\"Converted video saved as {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "def pad_audio_to_multiple_of_16(audio_path, target_fps=25):\n",
        "\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "    audio_duration = waveform.shape[1] / sample_rate  # Duration in seconds\n",
        "\n",
        "    num_frames = int(audio_duration * target_fps)\n",
        "\n",
        "    # Pad audio to ensure frame count is a multiple of 16\n",
        "    remainder = num_frames % 16\n",
        "    if remainder > 0:\n",
        "        pad_frames = 16 - remainder\n",
        "        pad_samples = int((pad_frames / target_fps) * sample_rate)\n",
        "        pad_waveform = torch.zeros((waveform.shape[0], pad_samples))  # Silence padding\n",
        "        waveform = torch.cat((waveform, pad_waveform), dim=1)\n",
        "\n",
        "        # Save the padded audio\n",
        "        padded_audio_path = \"padded_audio.wav\"\n",
        "        torchaudio.save(padded_audio_path, waveform, sample_rate)\n",
        "    else:\n",
        "        padded_audio_path = audio_path  # No padding needed\n",
        "\n",
        "    padded_duration = waveform.shape[1] / sample_rate\n",
        "    padded_num_frames = int(padded_duration * target_fps)\n",
        "\n",
        "    return padded_audio_path, padded_num_frames\n",
        "\n",
        "def create_video_from_image(image_path, output_video_path, num_frames, fps=25):\n",
        "    \"\"\"Convert an image into a video of specified length (num_frames at 25 FPS).\"\"\"\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(\"Error: Unable to read the image.\")\n",
        "        return None\n",
        "\n",
        "    height, width, _ = img.shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "    for _ in range(num_frames):\n",
        "        video_writer.write(img)\n",
        "\n",
        "    video_writer.release()\n",
        "    print(f\"Created video {output_video_path} with {num_frames} frames ({num_frames / fps:.2f} seconds).\")\n",
        "    return output_video_path\n",
        "\n",
        "\n",
        "# Очистка CUDA кэша перед запуском инференса\n",
        "if torch.cuda.is_available():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "\n",
        "output_display = widgets.Output() # Для вывода сообщений\n",
        "\n",
        "with output_display:\n",
        "    print(\"Starting inference with local files...\")\n",
        "\n",
        "    # Initialize paths to None\n",
        "    video_path_base = None\n",
        "    audio_path_padded = None\n",
        "    output_path_inference = None\n",
        "\n",
        "    try:\n",
        "        # Определяем полные пути к файлам\n",
        "        image_path_final = os.path.join(\"checkpoints\", image_file_name)\n",
        "        audio_path_final = os.path.join(\"checkpoints\", audio_file_name)\n",
        "\n",
        "        if not os.path.exists(image_path_final):\n",
        "            print(f\"Error: Image file not found at {image_path_final}\")\n",
        "        if not os.path.exists(audio_path_final):\n",
        "            print(f\"Error: Audio file not found at {audio_path_final}\")\n",
        "\n",
        "        if not os.path.exists(image_path_final) or not os.path.exists(audio_path_final):\n",
        "            print(\"Please ensure both image and audio files are in the 'checkpoints' folder and filenames are correct.\")\n",
        "        else:\n",
        "            print(f\"Using image: {image_path_final}\")\n",
        "            print(f\"Using audio: {audio_path_final}\")\n",
        "\n",
        "            # Assume default values for parameters, or define new widgets if needed for these.\n",
        "            seed_value = 1247\n",
        "            num_steps_value = 20\n",
        "            guidance_scale_value = 1.0\n",
        "            video_scale_value = 0.5\n",
        "            output_fps_value = 25\n",
        "\n",
        "            # Get audio duration with padding\n",
        "            with output_display:\n",
        "                print(\"Processing audio...\")\n",
        "                audio_path_padded, num_frames = pad_audio_to_multiple_of_16(audio_path_final, target_fps=25)\n",
        "                print(f\"Audio padded. Total frames for video: {num_frames}\")\n",
        "\n",
        "                # Create video from image\n",
        "                print(\"Creating base video from image...\")\n",
        "                video_path_base = \"generated_video.mp4\"\n",
        "                video_path_base = create_video_from_image(image_path_final, video_path_base, num_frames)\n",
        "\n",
        "                if video_path_base is None:\n",
        "                    print(\"Error: Failed to create video from image.\")\n",
        "                else:\n",
        "                    print(\"Running inference...\")\n",
        "                    output_path_inference = \"output_video.mp4\"\n",
        "                    perform_inference(video_path_base, audio_path_padded, seed_value, num_steps_value, guidance_scale_value, output_path_inference)\n",
        "\n",
        "                    print(\"Converting video FPS...\")\n",
        "                    final_output_video_path = convert_video_fps(output_path_inference, output_fps_value)\n",
        "\n",
        "                    if final_output_video_path is None:\n",
        "                        print(\"Error: Failed to convert video FPS.\")\n",
        "                    else:\n",
        "                        # Get dimensions from the original image for display scaling\n",
        "                        img_display = cv2.imread(image_path_final)\n",
        "                        if img_display is not None:\n",
        "                            height_display, width_display, _ = img_display.shape\n",
        "                        else:\n",
        "                            height_display, width_display = 720, 1280 # Default if image read fails\n",
        "\n",
        "                        print(\"Inference complete. Displaying output video:\")\n",
        "                        display(Video(final_output_video_path, embed=True, width=int(width_display * video_scale_value), height=int(height_display * video_scale_value)))\n",
        "    except Exception as e:\n",
        "        with output_display:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "    finally:\n",
        "        with output_display:\n",
        "            print(\"Cleanup complete.\")\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "                torch.cuda.ipc_collect()\n",
        "                gc.collect()\n",
        "            # Clean up temporary files created during this run\n",
        "            for path in [video_path_base, audio_path_padded, output_path_inference]:\n",
        "                if path and os.path.exists(path):\n",
        "                    os.remove(path)\n",
        "            print(\"Temporary files removed.\")\n",
        "\n",
        "display(output_display)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c754799e",
        "outputId": "652dfbfd-8c6b-4820-fb18-df9ad393f54f"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "output_filename = 'converted_25fps.mp4'\n",
        "\n",
        "if os.path.exists(output_filename):\n",
        "    files.download(output_filename)\n",
        "    print(f\"Файл {output_filename} был скачан.\")\n",
        "else:\n",
        "    print(f\"Файл {output_filename} не найден. Убедитесь, что он был успешно сгенерирован.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_73b9e56f-ba46-49ab-b9b2-1dc6524dcb4c\", \"converted_25fps.mp4\", 1247625)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файл converted_25fps.mp4 был скачан.\n"
          ]
        }
      ]
    }
  ]
}